{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEGqibOVgnTv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "import time\n",
        "from typing import Any, Tuple, Dict, Union\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add the Florence model directory to the Python path\n",
        "sys.path.append('/content/florence_model')\n",
        "\n",
        "# Import the custom Florence modules\n",
        "from configuration_florence2 import Florence2Config\n",
        "from modeling_florence2 import Florence2ForCausalLM\n",
        "from processing_florence2 import Florence2Processor\n",
        "\n",
        "# Add the SAM model directory to the Python path\n",
        "sys.path.append('/content/checkpoints')\n",
        "\n",
        "# Import SAM modules\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "# Set up device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Florence model setup\n",
        "FLORENCE_MODEL_PATH = \"/content/florence_model\"\n",
        "\n",
        "def load_florence_model(device: torch.device, model_path: str) -> Tuple[Any, Any]:\n",
        "    config = Florence2Config.from_pretrained(model_path)\n",
        "    model = Florence2ForCausalLM.from_pretrained(model_path, config=config, trust_remote_code=True).to(device).eval()\n",
        "    processor = Florence2Processor.from_pretrained(model_path, trust_remote_code=True)\n",
        "    return model, processor\n",
        "\n",
        "def run_florence_inference(\n",
        "    model: Any,\n",
        "    processor: Any,\n",
        "    device: torch.device,\n",
        "    image: Image.Image,\n",
        "    task: str,\n",
        "    text: str = None\n",
        ") -> Tuple[str, Dict]:\n",
        "    prompt = task + text if text else task\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        num_beams=3\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    response = processor.post_process_generation(generated_text, task=task, image_size=image.size)\n",
        "    return generated_text, response\n",
        "\n",
        "# SAM model setup\n",
        "SAM_CHECKPOINT = \"/content/checkpoints/sam2_hiera_large.pt\"\n",
        "SAM_CONFIG = \"/content/checkpoints/sam2_hiera_l.yaml\"\n",
        "\n",
        "def load_sam_image_model(device: torch.device, config: str = SAM_CONFIG, checkpoint: str = SAM_CHECKPOINT) -> SAM2ImagePredictor:\n",
        "    model = build_sam2(config, checkpoint, device=device)\n",
        "    return SAM2ImagePredictor(sam_model=model)\n",
        "\n",
        "def run_sam_inference(model: Any, image: np.ndarray, detections: sv.Detections) -> sv.Detections:\n",
        "    model.set_image(image)\n",
        "    bboxes = sorted(detections.xyxy, key=lambda bbox: bbox[0])\n",
        "    mask, score, _ = model.predict(box=bboxes, multimask_output=False)\n",
        "    if len(mask.shape) == 4:\n",
        "        mask = np.squeeze(mask)\n",
        "    detections.mask = mask.astype(bool)\n",
        "    return detections\n",
        "\n",
        "# Utility functions\n",
        "def fetch_image_from_url(image_url: str) -> Image.Image:\n",
        "    response = requests.get(image_url)\n",
        "    response.raise_for_status()\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "# Main processing function\n",
        "def process_image(\n",
        "    image_input: Union[str, Image.Image],\n",
        "    task_prompt: str,\n",
        "    text_prompt: str = None,\n",
        "    dilate: int = 0,\n",
        "    merge_masks: bool = False,\n",
        "    return_rectangles: bool = False,\n",
        "    invert_mask: bool = False\n",
        ") -> list:\n",
        "    # Load models\n",
        "    florence_model, florence_processor = load_florence_model(DEVICE, FLORENCE_MODEL_PATH)\n",
        "    sam_model = load_sam_image_model(DEVICE)\n",
        "\n",
        "    # Prepare image\n",
        "    if isinstance(image_input, str):\n",
        "        if image_input.startswith('http'):\n",
        "            image = fetch_image_from_url(image_input)\n",
        "        else:\n",
        "            image = Image.open(image_input)\n",
        "    else:\n",
        "        image = image_input\n",
        "\n",
        "    # Run Florence inference\n",
        "    _, result = run_florence_inference(\n",
        "        model=florence_model,\n",
        "        processor=florence_processor,\n",
        "        device=DEVICE,\n",
        "        image=image,\n",
        "        task=task_prompt,\n",
        "        text=text_prompt\n",
        "    )\n",
        "\n",
        "    # Create detections\n",
        "    detections = sv.Detections.from_lmm(\n",
        "        lmm=sv.LMM.FLORENCE_2,\n",
        "        result=result,\n",
        "        resolution_wh=image.size\n",
        "    )\n",
        "\n",
        "    images = []\n",
        "    if return_rectangles:\n",
        "        # Generate rectangle masks\n",
        "        image_width, image_height = image.size\n",
        "        merge_mask_image = np.zeros((image_height, image_width), dtype=np.uint8)\n",
        "        bboxes = sorted(detections.xyxy, key=lambda bbox: bbox[0])\n",
        "        for bbox in bboxes:\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            cv2.rectangle(merge_mask_image, (x1, y1), (x2, y2), 255, thickness=cv2.FILLED)\n",
        "            clip_mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
        "            cv2.rectangle(clip_mask, (x1, y1), (x2, y2), 255, thickness=cv2.FILLED)\n",
        "            images.append(clip_mask)\n",
        "        if merge_masks:\n",
        "            images = [merge_mask_image] + images\n",
        "    else:\n",
        "        # Generate segmentation masks using SAM\n",
        "        detections = run_sam_inference(sam_model, np.array(image.convert(\"RGB\")), detections)\n",
        "        if len(detections) == 0:\n",
        "            print(\"No objects detected.\")\n",
        "            return None\n",
        "        print(\"Masks generated:\", len(detections.mask))\n",
        "        kernel = np.ones((dilate, dilate), np.uint8)\n",
        "\n",
        "        for i in range(len(detections.mask)):\n",
        "            mask = detections.mask[i].astype(np.uint8) * 255\n",
        "            if dilate > 0:\n",
        "                mask = cv2.dilate(mask, kernel, iterations=1)\n",
        "            images.append(mask)\n",
        "\n",
        "        if merge_masks:\n",
        "            merged_mask = np.zeros_like(images[0], dtype=np.uint8)\n",
        "            for mask in images:\n",
        "                merged_mask = cv2.bitwise_or(merged_mask, mask)\n",
        "            images = [merged_mask]\n",
        "\n",
        "    if invert_mask:\n",
        "        images = [cv2.bitwise_not(mask) for mask in images]\n",
        "\n",
        "    return images\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/Jimin.jpeg\"\n",
        "task_prompt = \"<OPEN_VOCABULARY_DETECTION>\"\n",
        "text_prompt = \"person\"\n",
        "\n",
        "result_masks = process_image(\n",
        "    image_input=image_path,\n",
        "    task_prompt=task_prompt,\n",
        "    text_prompt=text_prompt,\n",
        "    dilate=10,\n",
        "    merge_masks=False,\n",
        "    return_rectangles=False,\n",
        "    invert_mask=False\n",
        ")\n",
        "\n",
        "# Display results\n",
        "if result_masks:\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, mask in enumerate(result_masks):\n",
        "        plt.subplot(1, len(result_masks), i+1)\n",
        "        plt.imshow(mask, cmap='gray')\n",
        "        plt.title(f\"Mask {i+1}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Overlay mask on original image\n",
        "    original_image = Image.open(image_path)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(original_image)\n",
        "    for mask in result_masks:\n",
        "        plt.imshow(mask, alpha=0.5, cmap='jet')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Masks overlaid on original image\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No masks were generated.\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
